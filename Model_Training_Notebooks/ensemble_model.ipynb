{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will be using the following ensemble methods\n",
    "* Averaging of probablitites\n",
    "* Weighted averaging of probablitites\n",
    "* Not using Majority voting as only 2 classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yamnet functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 13:57:37.421826: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-23 13:57:37.429520: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740299257.438516  146180 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740299257.441221  146180 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-23 13:57:37.450662: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Devices After Forcing CPU: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 🔹 Disable GPU before TensorFlow initializes\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"false\"  # Prevents GPU allocation\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# ✅ Verify TensorFlow is using only CPU\n",
    "print(\"TF Devices After Forcing CPU:\", tf.config.list_physical_devices())\n",
    "\n",
    "# ✅ Now load YAMNet\n",
    "yamnet_model = hub.load(\"https://tfhub.dev/google/yamnet/1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Load the pretrained YAMNet model\n",
    "with tf.device('/CPU:0'):  # Force CPU for TensorFlow\n",
    "    yamnet_model = hub.load(\"https://tfhub.dev/google/yamnet/1\")\n",
    "\n",
    "# Load our trained classifier model\n",
    "classifier_model = tf.keras.models.load_model(\"/home/maditya/Desktop/Front Era/Model1 :- YamNet/final_YamNet_classifier.h5\")\n",
    "\n",
    "# Class labels mapping\n",
    "class_names = [\"Cry\", \"Scream\", \"Normal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio_yamnet(audio_path, output_path=\"processed_audio.wav\", target_sr=16000):\n",
    "    \"\"\"\n",
    "    Converts input audio to WAV format if necessary, resamples to 16 kHz, 16-bit PCM, mono, and normalizes it.\n",
    "    If input is already a valid WAV file, skips unnecessary processing.\n",
    "    \"\"\"\n",
    "    # Check if file is already a WAV\n",
    "    if audio_path.lower().endswith(\".wav\"):\n",
    "        with sf.SoundFile(audio_path) as f:\n",
    "            if f.samplerate == target_sr and f.channels == 1 and f.subtype == \"PCM_16\":\n",
    "                print(\"Input is already a valid WAV file. Skipping conversion.\")\n",
    "                return audio_path  # No processing needed\n",
    "\n",
    "    # Otherwise, process the file\n",
    "    print(\"🔄 Processing audio to match required format...\")\n",
    "    \n",
    "    # Load audio file\n",
    "    y, sr = librosa.load(audio_path, sr=None, mono=True)  # Load as mono\n",
    "\n",
    "    # Convert to 16 kHz if needed\n",
    "    if sr != target_sr:\n",
    "        y = librosa.resample(y, orig_sr=sr, target_sr=target_sr)\n",
    "\n",
    "    # Normalize the audio (-1 to 1)\n",
    "    y = y / np.max(np.abs(y))\n",
    "\n",
    "    # Save as WAV with 16-bit PCM\n",
    "    sf.write(output_path, y, target_sr, subtype=\"PCM_16\")\n",
    "\n",
    "    return output_path  # Return processed file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### **2️⃣ Function to Extract YAMNet Embeddings**\n",
    "def extract_yamnet_embeddings(audio_path):\n",
    "    \"\"\"\n",
    "    Extracts YAMNet embeddings from an audio file and applies mean pooling.\n",
    "    \"\"\"\n",
    "    # Load audio file (YAMNet expects waveform in range [-1, 1])\n",
    "    y, sr = librosa.load(audio_path, sr=16000, mono=True)\n",
    "\n",
    "    # Ensure waveform is a float32 NumPy array\n",
    "    waveform = np.array(y, dtype=np.float32)\n",
    "\n",
    "    # Run YAMNet to get embeddings\n",
    "    _, embeddings, _ = yamnet_model(waveform)\n",
    "\n",
    "    # Mean pooling to get a (1024,) vector\n",
    "    mean_embedding = np.mean(embeddings.numpy(), axis=0)\n",
    "\n",
    "    return mean_embedding  # Shape: (1024,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_probs_yamnet(audio_file):\n",
    "    \"\"\"\n",
    "    1) Preprocess & load audio at 16kHz, mono, 16-bit PCM\n",
    "    2) Extract YAMNet embeddings (1024-d)\n",
    "    3) Predict with your Keras classifier -> get probabilities for [cry, scream, normal]\n",
    "    4) Return shape (3,) array\n",
    "    \"\"\"\n",
    "    # Step 1: Preprocess\n",
    "    processed_audio = preprocess_audio_yamnet(audio_file)  # from your code\n",
    "    \n",
    "    # Step 2: Extract YAMNet embeddings\n",
    "    embedding = extract_yamnet_embeddings(processed_audio)  # shape (1024,)\n",
    "\n",
    "    # Step 3: Keras model expects shape (1, 1024)\n",
    "    embedding = embedding.reshape(1, -1)\n",
    "\n",
    "    # Step 4: Predict probabilities\n",
    "    pred_probs = classifier_model.predict(embedding)  # shape (1, 3)\n",
    "    return pred_probs[0]  # shape (3,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wav2Vec2 functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import Wav2Vec2Model, Wav2Vec2FeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Wav2Vec2Model(\n",
       "  (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "    (conv_layers): ModuleList(\n",
       "      (0): Wav2Vec2GroupNormConvLayer(\n",
       "        (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "        (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "      )\n",
       "      (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n",
       "        (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (feature_projection): Wav2Vec2FeatureProjection(\n",
       "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): Wav2Vec2Encoder(\n",
       "    (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "      (conv): ParametrizedConv1d(\n",
       "        768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "        (parametrizations): ModuleDict(\n",
       "          (weight): ParametrizationList(\n",
       "            (0): _WeightNorm()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (padding): Wav2Vec2SamePadLayer()\n",
       "      (activation): GELUActivation()\n",
       "    )\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x Wav2Vec2EncoderLayer(\n",
       "        (attention): Wav2Vec2SdpaAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): Wav2Vec2FeedForward(\n",
       "          (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"facebook/wav2vec2-base-960h\"\n",
    "wav2vec2_model = Wav2Vec2Model.from_pretrained(model_name)\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
    "\n",
    "wav2vec2_model.eval()      # Inference mode\n",
    "wav2vec2_model.to(\"cpu\")   # or \"cuda\" if you have a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio_wav2vec2(input_path, output_path=\"temp_processed.wav\", target_sr=16000):\n",
    "    \"\"\"\n",
    "    Converts input audio to WAV (16kHz, 16-bit PCM, mono).\n",
    "    If it's already valid, you could skip re-saving. Here we always ensure consistency.\n",
    "    \"\"\"\n",
    "    # Load with librosa (auto-resamples if sr != None).\n",
    "    y, sr = librosa.load(input_path, sr=None, mono=True)\n",
    "\n",
    "    # If sample rate != target_sr, resample\n",
    "    if sr != target_sr:\n",
    "        y = librosa.resample(y, orig_sr=sr, target_sr=target_sr)\n",
    "\n",
    "    # Normalize to -1..1\n",
    "    if np.max(np.abs(y)) > 0:\n",
    "        y = y / np.max(np.abs(y))\n",
    "\n",
    "    # Save as 16-bit PCM WAV\n",
    "    sf.write(output_path, y, target_sr, subtype=\"PCM_16\")\n",
    "\n",
    "    return output_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_wav2vec2_embedding(audio_path):\n",
    "    \"\"\"\n",
    "    Loads audio at 16kHz, uses Wav2Vec2FeatureExtractor & Wav2Vec2Model to get (hidden_size,) embedding.\n",
    "    \"\"\"\n",
    "    # 1) Load audio at 16kHz\n",
    "    y, sr = librosa.load(audio_path, sr=16000, mono=True)\n",
    "\n",
    "    # 2) Convert waveform to Wav2Vec2 inputs\n",
    "    inputs = feature_extractor(y, sampling_rate=16000, return_tensors=\"pt\")\n",
    "\n",
    "    # 3) Forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = wav2vec2_model(**inputs)  # last_hidden_state shape: (1, seq_len, hidden_size)\n",
    "\n",
    "    # 4) Squeeze batch dimension => (seq_len, hidden_size)\n",
    "    last_hidden = outputs.last_hidden_state.squeeze(0)\n",
    "\n",
    "    # 5) Mean-pool over time => shape (hidden_size,)\n",
    "    mean_emb = last_hidden.mean(dim=0).cpu().numpy()\n",
    "\n",
    "    return mean_emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, input_dim=768, dropout_rate=0.3):\n",
    "        \"\"\"\n",
    "        A feed-forward neural network with Dropout and BatchNorm for regularization.\n",
    "        Adjust input_dim if your embeddings are not 768 in size.\n",
    "        \"\"\"\n",
    "        super(FFNN, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, 1024)\n",
    "        self.bn1 = nn.BatchNorm1d(1024)  \n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.bn2 = nn.BatchNorm1d(512)  \n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.dropout3 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "        self.dropout4 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.fc5 = nn.Linear(128, 3)  # Output for 3 classes\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        x = self.relu(self.bn4(self.fc4(x)))\n",
    "        x = self.dropout4(x)\n",
    "\n",
    "        x = self.fc5(x)  # Logits for classification\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FFNN(\n",
       "  (fc1): Linear(in_features=768, out_features=1024, bias=True)\n",
       "  (bn1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout1): Dropout(p=0.3, inplace=False)\n",
       "  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout2): Dropout(p=0.3, inplace=False)\n",
       "  (fc3): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout3): Dropout(p=0.3, inplace=False)\n",
       "  (fc4): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (bn4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout4): Dropout(p=0.3, inplace=False)\n",
       "  (fc5): Linear(in_features=128, out_features=3, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your trained weights\n",
    "device = torch.device(\"cpu\")  # or \"cuda\"\n",
    "acoustic_classifier = FFNN(input_dim=768, dropout_rate=0.3).to(device)\n",
    "acoustic_classifier.load_state_dict(torch.load(\"/home/maditya/Desktop/Front Era/Model2 :- Wav2Vec2/best_model.pth\", map_location=device))\n",
    "acoustic_classifier.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_probs_wav2vec2(audio_file):\n",
    "    \"\"\"\n",
    "    1) Preprocess & load audio at 16kHz\n",
    "    2) Extract Wav2Vec2 embeddings (768-d)\n",
    "    3) Forward pass through PyTorch FFNN -> get logits\n",
    "    4) Convert logits to probabilities via softmax -> shape (3,)\n",
    "    \"\"\"\n",
    "    # Step 1: Preprocess\n",
    "    processed_path = preprocess_audio_wav2vec2(audio_file, \"temp_processed.wav\")  # from your code\n",
    "\n",
    "    # Step 2: Extract Wav2Vec2 embedding\n",
    "    embedding = extract_wav2vec2_embedding(processed_path)  # shape (768,)\n",
    "\n",
    "    # Convert to torch tensor => shape (1, 768)\n",
    "    tensor_emb = torch.from_numpy(embedding).unsqueeze(0).float().to(device)\n",
    "\n",
    "    # Step 3: Get logits from your FFNN\n",
    "    with torch.no_grad():\n",
    "        logits = acoustic_classifier(tensor_emb)  # shape (1, 3)\n",
    "\n",
    "    # Step 4: Softmax -> probabilities\n",
    "    probs = torch.softmax(logits, dim=1).cpu().numpy()[0]  # shape (3,)\n",
    "    return probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble by Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_average(audio_file):\n",
    "    \"\"\"\n",
    "    1) Get probabilities from YAMNet & Wav2Vec2\n",
    "    2) Average them\n",
    "    3) Argmax -> final label\n",
    "    \"\"\"\n",
    "    p_yam = predict_probs_yamnet(audio_file)  # shape (3,)\n",
    "    p_wav = predict_probs_wav2vec2(audio_file)  # shape (3,)\n",
    "\n",
    "    # Average\n",
    "    p_ensemble = (p_yam + p_wav) / 2.0  # shape (3,)\n",
    "\n",
    "    final_class_idx = np.argmax(p_ensemble)\n",
    "    class_names = [\"cry\", \"scream\", \"normal\"]  # must match your training labels\n",
    "    predicted_label = class_names[final_class_idx]\n",
    "    confidence = p_ensemble[final_class_idx]\n",
    "\n",
    "    print(f\"\\nEnsemble (Averaging) Prediction => {predicted_label} (Confidence: {confidence:.2f})\")\n",
    "    return predicted_label, confidence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble by weighted averaging\n",
    "* YamNet is more accurate, hence give more weight to him"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_weighted(audio_file, alpha=0.6):\n",
    "    \"\"\"\n",
    "    Weighted ensemble:\n",
    "      p_ensemble = alpha * p_yam + (1 - alpha) * p_wav\n",
    "    If alpha > 0.5, YAMNet has more influence; if alpha < 0.5, Wav2Vec2 has more influence.\n",
    "    \"\"\"\n",
    "    p_yam = predict_probs_yamnet(audio_file)  # shape (3,)\n",
    "    p_wav = predict_probs_wav2vec2(audio_file)  # shape (3,)\n",
    "\n",
    "    # Weighted average\n",
    "    p_ensemble = alpha * p_yam + (1 - alpha) * p_wav  # shape (3,)\n",
    "\n",
    "    final_class_idx = np.argmax(p_ensemble)\n",
    "    class_names = [\"cry\", \"scream\", \"normal\"]\n",
    "    predicted_label = class_names[final_class_idx]\n",
    "    confidence = p_ensemble[final_class_idx]\n",
    "\n",
    "    print(f\"\\nEnsemble (Weighted α={alpha}) Prediction => {predicted_label} (Confidence: {confidence:.2f})\")\n",
    "    return predicted_label, confidence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = \"/home/maditya/Desktop/Front Era/Other/Augmented_Dataset/Scream/scream_06_aug1.wav\"\n",
    "\n",
    "# 1) Ensemble by simple averaging\n",
    "label_avg, conf_avg = ensemble_average(audio_file)\n",
    "\n",
    "# 2) Ensemble by weighted averaging (e.g., alpha=0.7 => 70% YAMNet, 30% Wav2Vec2)\n",
    "label_wt, conf_wt = ensemble_weighted(audio_file, alpha=0.7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5% of files from each class have been copied to Test_data_ensemble.\n",
      "CSV file '/home/maditya/Desktop/Front Era/Ensemble/augmented_dataset.csv' has been created with the mapping of audio files to labels.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import csv\n",
    "import random\n",
    "\n",
    "# Define paths\n",
    "source_folder = \"/home/maditya/Desktop/Front Era/Ensemble/Augmented_Dataset\"\n",
    "destination_folder = 'Test_data_ensemble'\n",
    "csv_file_path = \"/home/maditya/Desktop/Front Era/Ensemble/augmented_dataset.csv\"\n",
    "\n",
    "# Create the destination folder if it doesn't exist\n",
    "if not os.path.exists(destination_folder):\n",
    "    os.makedirs(destination_folder)\n",
    "\n",
    "# Initialize the CSV file\n",
    "with open(csv_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['filename', 'label'])  # Write the header\n",
    "\n",
    "    # Iterate through each class folder\n",
    "    for class_name in ['Cry', 'Normal', 'Scream']:\n",
    "        class_folder = os.path.join(source_folder, class_name)\n",
    "        files = os.listdir(class_folder)\n",
    "        \n",
    "        # Calculate 5% of the files\n",
    "        num_files_to_copy = max(1, int(len(files) * 0.05))  # Ensure at least 1 file is copied\n",
    "        selected_files = random.sample(files, num_files_to_copy)\n",
    "        \n",
    "        # Copy the selected files and write to CSV\n",
    "        for file_name in selected_files:\n",
    "            source_file = os.path.join(class_folder, file_name)\n",
    "            destination_file = os.path.join(destination_folder, file_name)\n",
    "            shutil.copy(source_file, destination_file)\n",
    "            \n",
    "            # Write to CSV\n",
    "            writer.writerow([file_name, class_name])\n",
    "\n",
    "print(f\"5% of files from each class have been copied to {destination_folder}.\")\n",
    "print(f\"CSV file '{csv_file_path}' has been created with the mapping of audio files to labels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting labels in test_Data_ensemble.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded labels have been added to the new CSV file: test_data_ensemble_encoded.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Define the CSV file path\n",
    "csv_file_path = \"/home/maditya/Desktop/Front Era/Ensemble/test_data_ensemble.csv\"\n",
    "updated_csv_file_path = 'test_data_ensemble_encoded.csv'\n",
    "\n",
    "# Define the label encoding mapping\n",
    "label_encoding = {'Cry': 0, 'Scream': 1, 'Normal': 2}\n",
    "\n",
    "# Read the original CSV and write to a new CSV with encoded labels\n",
    "with open(csv_file_path, mode='r') as infile, open(updated_csv_file_path, mode='w', newline='') as outfile:\n",
    "    reader = csv.reader(infile)\n",
    "    writer = csv.writer(outfile)\n",
    "    \n",
    "    # Read the header and add a new column for encoded labels\n",
    "    header = next(reader)\n",
    "    header.append('encoded_label')  # Add the new column\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    # Process each row\n",
    "    for row in reader:\n",
    "        filename, label = row\n",
    "        encoded_label = label_encoding[label]  # Get the encoded value\n",
    "        row.append(encoded_label)  # Add the encoded label to the row\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"Encoded labels have been added to the new CSV file: {updated_csv_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
