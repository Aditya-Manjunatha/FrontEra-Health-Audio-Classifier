# ğŸ™ï¸ Developing an Ensemble Model for Detecting Infant Cries, Screams, and Normal Utterances  

## ğŸ“Œ Project Overview  

The goal of this project is to **classify audio files into one of three categories**:  
1ï¸âƒ£ **Cry**  
2ï¸âƒ£ **Scream**  
3ï¸âƒ£ **Normal Speech**  

To accomplish this task, we **fine-tune two pretrained models** and create an **ensemble model** for improved performance:  

### ğŸ”¹ **Pretrained Models Used:**  
âœ… **[YAMNet](https://www.researchgate.net/figure/YAMNet-Body-Architecture-Conv-Convolution-dw-Depthwise-pw-Pointwise_fig2_354549303)** - A deep neural network for sound classification trained on AudioSet.  
âœ… **[Wav2Vec2](https://jonathanbgn.com/2021/09/30/illustrated-wav2vec-2.html)** - A self-supervised model for speech and sound processing.  

Once fine-tuned, we create **three inference pipelines**:  
1ï¸âƒ£ **YAMNet-based classifier**  
2ï¸âƒ£ **Wav2Vec2-based classifier**  
3ï¸âƒ£ **Ensemble model** combining both predictions  



## ğŸ“Œ Setting Up the Environment  

Before diving into data preprocessing and model training, we need to **set up the environment** to ensure smooth execution of the code.  



### **1. Create a Virtual Environment**  
It is **recommended** to use a virtual environment to keep dependencies organized and avoid conflicts.  

#### **For Linux & macOS:**  
```bash
python3 -m venv env
source env/bin/activate
```
## **ğŸ“Œ Setting Up the Repository & Organizing Files**  

Before running inference, follow these steps to **set up the repository, download necessary files, and organize them properly**.

---

### **1ï¸âƒ£ Clone the Repository & Set Up the Folder**  
Run the following commands to create an `AudioClassifier` folder and clone the repo inside it:  

```bash
mkdir AudioClassifier && cd AudioClassifier

# Clone the repository
git clone https://github.com/Aditya-Manjunatha/FrontEra-Health-Audio-Classifier.git .

# Verify that required files are in place
ls -l
```
### **2ï¸âƒ£ Download & Store Wav2Vec2 Locally**  
To avoid downloading the Wav2Vec2 model every time, store it inside the folder :
```bash
mkdir models
huggingface-cli download facebook/wav2vec2-base-960h --cache-dir ./models/wav2vec2/

```
### **3ï¸âƒ£ Install Dependencies** 

A requirements.txt file is provided to install the libraries to be installed

```bash
pip install -r requirements.txt
```
### **4ï¸âƒ£ Move your Audio Files to Folder** 
If you already have a folder containing your test audio files, move it into AudioClassifier/:
```bash
mv /path/to/your/test_audio/ AudioClassifier/
```
If you donâ€™t have a test_audio folder, create it manually and add your test .wav files inside:
```bash
mkdir AudioClassifier/test_audio
mv /path/to/your_audio.wav AudioClassifier/test_audio/
```

### **5ï¸âƒ£ Verify Folder Structure**
Verify that the folder looks like this now
```bash
AudioClassifier/
â”‚â”€â”€ YamNet_infer.py
â”‚â”€â”€ Wav2Vec2_infer.py
â”‚â”€â”€ Ensemble_infer.py
â”‚â”€â”€ final_YamNet_classifier.h5
â”‚â”€â”€ best_model.pth
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ report.pdf
â”‚â”€â”€ models/
â”‚   â””â”€â”€ wav2vec2/  # Locally downloaded Wav2Vec2 model
â”‚â”€â”€ test_audio/
â”‚   â”œâ”€â”€ sample1.wav
â”‚   â”œâ”€â”€ sample2.wav
â”‚   â””â”€â”€ ...
â”‚â”€â”€ Model_Training_Notebooks/  # Jupyter notebooks used for training
â”‚   â”œâ”€â”€ YamNet_Training.ipynb
â”‚   â”œâ”€â”€ Wav2Vec2_Training.ipynb
â”‚   â”œâ”€â”€ Data_Preprocessing.ipynb
â”‚â”€â”€ Datasets/  # Data used for training
â”‚   â”œâ”€â”€ Cry/
â”‚   â”‚   â”œâ”€â”€ cry_01.wav
â”‚   â”‚   â”œâ”€â”€ cry_02_aug.wav
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ Scream/
â”‚   â”‚   â”œâ”€â”€ scream_01.wav
â”‚   â”‚   â”œâ”€â”€ scream_02_aug.wav
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ Normal/
â”‚   â”‚   â”œâ”€â”€ normal_01.wav
â”‚   â”‚   â”œâ”€â”€ normal_02_aug.wav
â”‚   â”‚   â””â”€â”€ ...
```



## **4. Running Inference on an Audio File**  

Once the **model weights** and **inference scripts** are in place, you can **run inference on any test audio file** to classify it as **Cry, Scream, or Normal**.

**NOTE :- Ensure you have cd'd AudioClassifier Folder**
### **ğŸ”¹ Step 1: Run an Inference Script**  
After placing the inference scripts and model weights in the correct directory, choose which model to run and execute the corresponding script.

#### **Run YAMNet Inference**  
```bash
python YamNet_infer.py
```
#### **Run Wav2Vec2 Inference**  
```bash
python Wav2Vec2_infer.py
```
#### **Run Ensemble Inference**  
```bash
python Ensemble_infer.py
```

### **ğŸ”¹ Step 3: Provide the Audio File Path**  
Once you run the script for `YamNet_infer.py` OR `Wav2Vec2_infer.py`, it will prompt you to enter the path of an audio file.

Example :
```bash
Enter the path of the audio file: /test_audio/sample1.wav

```

If you run the script `Ensemble_infer.py` it will ask you which type of ensemble method you want . 

Example :
```bash
Choose Ensemble Method:
1 - Averaging
2 - Weighted Averaging (default weight: Î± = 0.7)
Enter your choice (1 or 2): 1
```
Once you select the method, it will ask for the audio file path as seen before

### **ğŸ”¹ Step 4: Get the Prediction** 
Wait for the model to process the audio file and return the predicted label.

Example Output (YAMNet):

```bash
ğŸ”„ Processing audio to match required format...
âœ… Extracting YAMNet embeddings...
âœ… Running classification model...
Predicted Class: Scream (Confidence: 0.94)
```

Example Output (Ensemble Model - Average & Weighted Predictions):

```bash
ğŸ”„ Processing audio to match required format...
âœ… Extracting YAMNet embeddings...
âœ… Extracting Wav2Vec2 embeddings...
âœ… Running classification models...

ğŸ”¹ Ensemble (Averaging) Prediction => Scream (Confidence: 0.92)
ğŸ”¹ Ensemble (Weighted Î±=0.7) Prediction => Scream (Confidence: 0.95)

ğŸŸ¢ Final Predictions:
âœ… Ensemble (Averaging) â†’ Scream
âœ… Ensemble (Weighted) â†’ Scream

```
---
## **5. Diving Deeper - Data**  

Now that the setup is complete, we can explore the **data collection, preprocessing, and augmentation strategies** used in this project.  



### **ğŸ”¹ Data Collection**  
We gathered audio data from multiple sources to ensure a diverse and balanced dataset:  

1ï¸âƒ£ **[Infant Cry Audio Corpus](https://www.kaggle.com/datasets/warcoder/infant-cry-audio-corpus)** â€“ Contains various infant cry recordings.  
2ï¸âƒ£ **[Infant's Cry Sound Dataset](https://data.mendeley.com/datasets/hbppd883sd/1)** â€“ Another dataset featuring different infant cries.  
3ï¸âƒ£ **[Human Screaming Detection Dataset](https://www.kaggle.com/datasets/whats2000/human-screaming-detection-dataset)** â€“ Includes human screams from different environments.  
4ï¸âƒ£ **[Common Voice Dataset](https://www.kaggle.com/datasets/mozillaorg/common-voice)** â€“ Provides normal human speech recordings.  



### **ğŸ”¹ Audio Preprocessing**  
Upon reviewing the **YAMNet** and **Wav2Vec2** documentation, I identified the required **audio specifications**:  

âœ… **WAV Format** 

âœ… **16 kHz sample rate**  

âœ… **16-bit PCM encoding**  

âœ… **Mono-channel audio**  

All collected audio files were **preprocessed accordingly** using **Librosa** and **SoundFile**, ensuring compatibility with the models. Preprocessing steps included:  

- **Resampling** audio to **16 kHz** (if different).  
- **Converting to PCM16** format.  
- **Ensuring mono-channel audio** for consistency.  



### **ğŸ”¹ Data Augmentation**  

Since our dataset had **limited samples**, we applied **data augmentation** techniques to generate additional training examples and improve model generalization.  

ğŸ“Œ **Class Distribution Before Augmentation:**  
- ğŸ¼ **Cry Class** â†’ **565 samples**  
- ğŸ“¢ **Scream Class** â†’ **862 samples**  
- ğŸ—£ï¸ **Normal Utterance Class** â†’ **4,067 samples** (sampled **~800**)  

To balance the dataset, we randomly applied one of the following augmentations to each sample to **create more datapoints** 

âœ… **Time Stretching** â€“ Slowing down or speeding up the audio without changing pitch.  
âœ… **Pitch Shifting** â€“ Raising or lowering the pitch while maintaining tempo.  
âœ… **Background Noise Addition** â€“ Adding white noise or random environmental sounds.  
âœ… **Volume Perturbation** â€“ Slightly increasing or decreasing audio loudness.  
âœ… **Time Shifting** â€“ Introducing slight delays to modify the starting point.  

These augmentations **helped improve model robustness** by exposing it to **varied audio conditions** while maintaining the core characteristics of the target classes.  


### **ğŸ”¹ Dataset Readiness**  
After preprocessing and augmentation, our dataset was **fully prepared** after combining both the original and the augmented datapoints. This dataset is then used for **fine tuning** **YAMNet** and **Wav2Vec2**.

## **5.1. Diving Deeper - Fine-Tuning**  

### **ğŸ”¹ What is Fine-Tuning?**  
Fine-tuning is the process of **adapting a pretrained model** to a **specific task** by training it on a smaller, task-specific dataset. There are **two common approaches** to fine-tuning:  

1ï¸âƒ£ **Changing the classifier head** â€“ Replacing the final classification layer while keeping the pretrained feature extractor **frozen**.  
2ï¸âƒ£ **Unfreezing layers & training** â€“ Unfreezing some (or all) layers and training them on new data.  

Since our dataset was **limited in size**, and we had **compute constraints**, we chose the **first approach**â€”using pretrained models as **feature extractors** and training our own classifier on the extracted embeddings.  



## **ğŸ“Œ 5.1.1 Fine-Tuning YAMNet**  

### **ğŸ”¹ Understanding YAMNetâ€™s Architecture**  
YAMNet is a **deep audio classification model** trained on **AudioSet**, capable of recognizing **521 audio event classes**. Instead of modifying its classifier, we **leveraged its embeddings**:  

ğŸ“Œ **Key Observation:**  
- In YAMNet, the **second-to-last layer** outputs a **1024-dimensional embedding** for each audio frame.  
- These embeddings are then passed into a **512-class classifier** in the original model.  

### **ğŸ”¹ Extracting Embeddings**  
To **adapt YAMNet to our task**, we:  
âœ… Passed each audio file through YAMNet to **extract embeddings**.  
âœ… Applied **mean pooling** to aggregate frame-level embeddings into a **single 1024-dimensional vector per file**.  
âœ… Used these **mean-pooled embeddings** as input features for our classifier.  

### **ğŸ”¹ Custom FFNN Classifier**  
We designed a **Feedforward Neural Network (FFNN)** to classify the extracted YAMNet embeddings.  

ğŸ”¹ **FFNN Architecture:**  
- **Input Layer** â†’ `1024 neurons` (YAMNet embedding size)  
- **Hidden Layers** â†’ `3 layers` with **ReLU activation** and **dropout** for regularization  
- **Output Layer** â†’ `3 neurons` (Softmax activation for `Cry`, `Scream`, `Normal`)  

## **ğŸ“Œ 5.1.2 Fine-Tuning Wav2Vec2**  

### **ğŸ”¹ Understanding Wav2Vec2â€™s Architecture**  
Wav2Vec2 is a **self-supervised speech model** trained on large-scale speech datasets. It consists of:  

ğŸ”¹ **Key Components:**  
- **Feature Encoder** â†’ Convolutional layers that extract raw audio features.  
- **Transformer Encoder** â†’ Outputs a `768-dimensional` feature vector for each audio frame.  
- **Final Classification Head** (which we **ignored**).  


### **ğŸ”¹ Extracting Embeddings**  
To **adapt Wav2Vec2 to our task**, we:  
âœ… Passed each audio file through **Wav2Vec2â€™s transformer encoder** to **extract 768-dimensional embeddings per frame**.  
âœ… Applied **mean pooling** to obtain a **single 768-dimensional vector per file**.  
âœ… Used these **mean-pooled embeddings** as input features for our classifier.  


### **ğŸ”¹ Custom FFNN Classifier**  
Similar to YAMNet, we designed a **FFNN classifier** for the Wav2Vec2 embeddings.  

ğŸ”¹ **FFNN Architecture:**  
- **Input Layer** â†’ `768 neurons` (Wav2Vec2 embedding size)  
- **Hidden Layers** â†’ `3 layers` with **BatchNorm, ReLU activation, and Dropout**  
- **Output Layer** â†’ `3 neurons` (Softmax activation for `Cry`, `Scream`, `Normal`)  

## **ğŸ“Œ 5.2 Diving Deeper - Training and Hyperparameter Tuning**  

### **ğŸ”¹ Hyperparameter Tuning with K-Fold Cross-Validation**  
To select the optimal hyperparameters, we initialized **5 different hyperparameter combinations** and performed **K-Fold Cross-Validation** on each.  

âœ… **K-Fold Cross-Validation** was used to ensure that the model **generalizes well** and does not overfit.  
âœ… For each fold, we **trained and validated** on different splits and **computed key metrics** like **accuracy, precision, recall, and F1-score**.  
âœ… The **detailed results** of each hyperparameter combination are provided in the **REPORT**.  



### **ğŸ”¹ Selecting the Best Hyperparameters**  
After evaluating the **average accuracy across all folds**, we selected the **best-performing hyperparameter combination** and used it to train the final model.  

ğŸ“Œ The **best hyperparameter combination** was then used in the **main training loop** to train the model on the **entire 70% training set**.  



### **ğŸ”¹ Final Model Training**  
Once the best hyperparameters were chosen:  
âœ… The model was trained on the **full training set (70%)**.  
âœ… Metrics such as **training accuracy, validation accuracy, and loss curves** were monitored to prevent overfitting.  
âœ… The **final trained model** was saved for inference and testing.  



### **ğŸ”¹ Final Model Testing**  
After training, the model was evaluated on the **held-out test set (15%)**.  
âœ… **Test accuracy, precision, recall, and F1-score** were computed.  
âœ… A **detailed performance analysis** with visualizations like **confusion matrices and ROC curves** is provided in the **REPORT**.  

---






